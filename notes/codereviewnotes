## Intro

As we write more code to support scientific research, we'd like to put a system in place that can be shown to reduce the number of bugs in that code, thereby upholding a high standard of quality for the underlying science - and we'd like to do so without an overwhelming investment in time or resources. What's more, we would like to further minimize the amount of time we as scientists spend writing code, by making the sharing, resue, and collaborative development of scientific code both viable and convenient. How can we design a process that speaks to all these concerns?

We can start by looking to our traditions surrounding writing manuscripts. The process of manuscript review gives us two major things: a set of customs and expectations for scholarly writing, that participants in a field can at least roughly adhere to even before the review process beings; and a formal and accountable process for reviewing a manuscript once it is written, to push it towards standards of excellence. The process of code review consists of similar parts: first, we must establish a set of basic customs that we believe will predispose our code to quality (the equivalent of good grammar and conventional structure); and second, we must formulate a process for examining code, to see if it not only meets these basic standards, but rises to a minimum level of quality, the details of which we are free to define, but must define clearly.

Which all begs the question: what customs and standards should we define for our code? What makes code 'good'? If our goal is to catch & eliminate bugs during development, we must be able to clearly define what we intend a piece of code to do, and present it in a way that is legible enough for a reader to assess whether it (likely) lives up to those standards. And if our goal is to enable collaboration on and reuse of code, we must find a way to build rapport with our collaborators, help them understand the goals of our code, and give them a simple but precise way of communicating back to us what contributions they would like to make to support those goals.

In other words, we need a procedure for clearly communicating about code, in order to help make errors more obvious, and smooth the process of working together - and this is exactly what a pattern of code review sets out to do. In what follows, we'll examine the three major steps in this pattern:

 - Defining the goals for a code project
 - Defining the anatomy of a contribution that lends itself to legible code and effective review
 - Laying out the process of actually reviewing code, in a step-by-step fashion designed to retain effectiveness while minimizing time committment.



## Desigining your project

All review processes require a standard to review against; and if our primary goal is to catch bugs and ensure our code is doing the tasks intended, we need to lay out just what those tasks are. By answering the question 'what are we trying to achieve?', we set a finite limit on what should and should not be included in our project; this is called defining a scope, and it gives us a critical frame of reference for judging new contributions. Many a coding project has failed due to 'feature creep' - the phenomenon of endlessly adding just one more interesting idea to the code, regardless of original intentions. Make no mistake: there is an infinite number of interesting ideas in the world. Trying to roll them all up into one project results in a monstrously huge codebase that is impossible to understand, maintain or use.

Once we have that high-level statement of what we're trying to achieve, we can begin to sketch out a rough plan for our code - where do we want to start from, and what are the steps we expect to have to go through to get to our goal? At this stage, we aren't trying to lay out every single implementation detail; rather, we want a roadmap that lets us do two things:
 - Separate and simplify the steps to achieving our goals. After sketching a first draft of a plan, points at which to divide a problem up often become obvious: these are extremely valuable opportunities! By taking advantage of opportunities to split a problem up, we turn a complicated analysis into a series of simpler tasks - and the smaller and simpler each of these tasks are, the more likely you'll be able to catch bugs in them as you examine and use the code. Furthermore, as tasks are split up, they tend to become more generic, and a piece of code that accomplishes a more generic goal is more likely to be able to be reused in the future. This is the practice of writing modular code: code that is built out of many small, simple pieces, that are easy to debug and easy to use.
 - Communicate our goals to our collaborators. A relatively simple, high-level roadmap is a valuable tool for helping our colleagues understand how to use and contribute to our software. Consider the following problem: a newcommer wants to participate in your project. What's the best possible understanding of the project we can give her within five minutes of her arrival? I call this a 'five minute plan' - a clear, high-level roadmap that can orient a collaborator quickly, so they can begin to make useful contributions as soon as possible.

So, our two goals at this stage are:
 - Define the scope of our project, so we can make judgements on what should and shouldn't be included; answer the question, 'what are we trying to achieve?'
 - Sketch a high-level plan of our project, so we can split it up into simple tasks and communicate the lay of the land to collaborators quickly and easily; this 'five minute plan' should be digestible by a newcommer inside of five minutes.


### Example

A bacteriophage ('phage' for short) is a virus that infects bacteria. In order to stave off these infections, bacteria keep chunks of phage genome in their own DNA, to serve as templates to help them recognize phages when they attack. An interesting study in immunology, then, would be to examine and compare repositories of phage and bacteria DNA to see where exactly these chunks of phage genome appear in bacterial DNA, so we can understand better this genetic archival process.

Sounds like a job for some code! We can immediately write down a simple goal for this project:

"Download phage and bacteria genomes from open databases, and identify the nature and position of matches to phage DNA within bacteria DNA."

By writing down a simple statement that circumscribes the scope of our project, we have done ourselves a great service: now, when an eager colleague comes along who is interested in something similar but not quite the same, and suggests we add functionality to look for instances of horizontal gene transfer between bacteria, we can easily judge that this would be out of scope, point to our goals, and say politely but resolutely, 'no'. Would code to identify horizontal gene transfer be interesting and useful? Absolutely! But the mistake we are trying to avoid is rolling up too much complication into one project. By keeping things focused and simple, maintaining and using our code remains fast and easy, and bugs remain easier to catch. That horizontal gene transfer idea is an excellent project in its own right - it deserves to stand on its own, in its own easily maintained project.

Now that we know what we're trying to accomplish, we can start to imagine some simple steps to get there. An enormously useful tool in this process is a simple flow chart. We can turn our original statement into a simple flow chart as follows:

[1]

Once we've drawn even this simple diagram, we can start to think more clearly about our project. Consider, for example, the 'compare and report' step - what does this mean? What will be involved in achieving this? Perhaps the place we're getting our data from reports it in some messy or awkward manner; we'll have to tidy it up into a form more useful for our purposes. Then, we want to search for matches between phage and bacteria DNA, and report those matches; the 'compare and report bubble' breaks down into something of the form:

[2]

Now, some more questions arise: will the cleaning process be the same for the phage and bacteria raw data? Probably we should consider splitting these up into separate cleaning processes for each type. And how exactly do we intend to do that 'search for matches' step - searching through data can be complicated, but we can simplify the process by using a database with built-in search functionality. Our flow chart becomes something like:

[3]

At this point, something very powerful has occurred; we've split the process up enough, that a major logical division has emerged:

[4]

Once that database is populated, we can carry on with our phage / bacteria DNA matching search. But we've also created a useful aretefact (the database) that can be reused for other tasks in the future - for example, we can tell that keen colleague that while we aren't going to write horizontal gene transfer detection algorithms into our project, she's welcome to use our database as a starting point to do it herself. What's more, we've created a logical division of labor in our own project; one grad student can work on getting the data downloaded, cleaned up and populated in the database, while another completely independently figures out how to search for matches and report the results. By splitting tasks up, we've made them simpler, easier to reuse, and easier to implement. And when we get down to the business of reviewing code, we've set ourselves up to answer much simpler questions; deciding if a piece of code is doing one simple bubble in our flow chart is much easier than deciding if it's contributing to the whole project in some poorly defined sense. Finally, this flow chart is a great example of a 'five minute plan' - a new collaborator can look at it, and quickly grasp the basic idea of what's going on.

### Don't Overdo It (The Plan Will Change)

Having a clear plan is hugely helpful in communicating with collaborators, keeping projects well-managed, and setting the task of code review up to be fast and easy. But don't overdo it. The realities of most software projects is that needs and goals are emergent - they become clearer as the project progresses, and this is especially true when you throw scientific discovery into the mix. Planning every last detail of a project up front is called waterfall design, and while it seems like it would be a logical extension of planning to make sure that plan is as detailed and complete as possible, it usually backfires. An exquisitly detailed plan written up-front almost never predicts all the problems and discoveries that emerge during the development process; and then to change such a detailed document to reflect these emergent details is a huge amount of work, which people rarely make time for in practice. The result is a project plan that is out of sync with the realities of the project, that ends up misleading and confusing newcomers, and misrepresenting the project as a whole.

Therefore, keep this plan simple and high-level, so that it can be changed easily (do not, however, let people bully you into changing the plan without due cause - remember, part of the point of this step is to help you judge what does and does not belong in this project; but that filter should be your deciding authority, justified and communicated by the plan, not the cumbersome nature of an overly-detailed strategy document). A good guiding principle is to answer the question: 'what is the best introduction to this project I can give a newcommer in five minutes?'

### Summary

In order to set up the process of code review and enable effective collaboration, start by wirting down answers to two questions:

 - What is the overall goal of this project? This gives you a mission statement that helps control the scope of the project, and make judgements on what to include.
 - What is the 'five-minute plan' for this project? Sketch out a plan for how this project is going to achieve its goals, possibly as a flow chart, that can be communicated to a new collaborator in under five minutes; look for opportunities to split tasks up into simple, independent chunks.




## Defining a good contribution

Now that we have a road map for our project, we're almost ready to start coding. But first, it will save us a lot of time and effort later if we set up some ground rules for what a good contribution to our project looks like. These ground rules should be designed to make contributions as easy as possible for a reviewer to understand and assess, and to eliminate simpler mistakes before the contribution is proposed in the first place; these guidelines ideally enforce a minimum standard of quality, while simultaneously minimizing the amount of time the reviewer (probably yourself) needs to spend combing through code. Think of them as the grammar of a contribution, and the set of basic customs that make something familliar and easily digestable; they are the minimum standards that people should self-check for before a contribution is made.

In what follows, I'll lay out and explain some common standards. The details may need to change to suit your project, but the overall structure typically remains the same.

### Communication

There is no substitute for a good conversation. Before they begin on a contribution, encourage collaborators to have a discussion with you (or whoever is maintaining the code) about what exactly it is that they'd like to do. Will the proposed new code contribute to the existing plan for the project, fix an existing problem, or change the course or nature of the project in some way? Will it fit with the existing work well? An up-front discussion about intentions helps to avoid misunderstandings and wasted effort down the line, and lets this whole process start from a place of clear communication and understanding.

This is where that five-minute plan you sketched out when you were preparing your project will come in handy. If this is a proposal for new code, ask the contributor exactly which step of the plan they plan on contributing to, and how they indend on achieving that; or, if their proposal seems out of scope, point to that road map as an explanation of why the proposal isn't the right fit. The simple act of getting people to articulate how their plans fit with yours will often bring to light problems and challenges before they turn into code contributions that aren't quite right.

Software collaboration platforms like GitHub provide facilities for these conversations in the form of an 'issue tracker' - use them. By asking a collaborator to open an issue (which works essentially like a forum or message board) to describe and discuss their intentions before they get started, not only can we start from a place of mututal understanding and avoid wasted effort on all sides, but a trail of documentation then exists; when the code contribution finally comes in, you'll have a clear record of the conversation around it that will help you decide if the new code indeed addresses the issue at hand.

That being said - a quick water cooler chat is far better than nothing. Tools aside, what really matters is establishing a clear mutual understanding of what a new piece of code is supposed to do.

### Submission Guidelines

The code review process is greatly expedited by standardization. By setting up some basic guidelines for submissions, we can enable contributors to self-check for basic requirements, and ensure that the code we have to review comes in an easily digestible format. Each project may need slightly different submission guidelines to suit its needs, but here are a set of common ones to start from; whatever list you come up with, put it in a very obvious and prominent place in your repository - probably in the README file at the top level of the project, under a clear heading of 'Submission Guidelines'.

 - Maximum submission length of 400 lines. It's impossible to review thousands of lines of code in a sitting; [this study](http://smartbear.com/smartbear/media/pdfs/wp-cc-11-best-practices-of-peer-code-review.pdf) recommends no more than 400 lines at once. Going beyond this reduces the amount of bugs we can successfully detect, while making the amount of time needed for code review blow up. If more changes than this are required, they should almost certainly be in separate contributions.
 - Maximum function length of 50 lines. The shorter and simpler a function is, the easier it is for the reviewer to confirm that it in fact does what it's supposed to; long, complicated functions are difficult to assess, and rarely lend themselves to reuse down the line.
 - Code must be automatically mergable. Version control systems will report if a new contribution can be automatically merged into the existing code, or if a conflict exists. Make it squarely the responsibility of the conrtributor to resolve all conflicts before submission, so you aren't overwhelmed with fitting things together yourself.
 - All tests must pass. One of the other benefits of insisting on small, modular functions, is that they lend themselves better to writing automated unit tests to ensure their helthy functioning. If your project contains a test suite, insist that all tests must still pass after the inclusion of a new piece of code, as an automatic cross-check to ensure nothing has been broken by the changes.
 - All functions must be tested. Similarly to the above, if a contribution creates a new function, insist that it have at least one or two tests in the test suite that check that it is doing what it's supposed to. By requiring contributors to write tests as they go along, the effort of building up a test suite remains managable and distributed.
 - All functions must be documented. Every function should be accompanied by a comment indicating, at a minimum, what the function is supposed to do, what its inputs are, and what it will return. Again - insisting contributors write these as they go along prevents the task of generating documentation from becomming overwhelmingly time consuming.
 - Respect the style guide. Most programming languages have an agreed upon guide for programming style, designed to make code as readable and standardized as possible. What's more, free utilities exist that will automatically check for compliance with these rules (see [pep8](https://pypi.python.org/pypi/pep8) for Python or [jslint](https://www.npmjs.com/package/jslint) for JavaScript, for example). Indicate to your contributors what style guide and checking tool you are using, and demand they submit code that is strictly compliant to these standards.
 - Indicate what issue this contribution addresses. As discussed above, much sadness is avoided by talking about plans for code contribution beforehand. Ask your contributors to link to the thread in the issue tracker that led to this contribution, so that you can quickly see what this contribution is trying to address.
 - Summarize and annotate changes. Ask your contributors to write up a very brief, point form description of how their contribution goes about achieving its intended task. By asking your contributors to articulate the strategy behind their work, not only do you get an easily-understood description of the contribution, but there will be less bugs on average! By thinking about what they've done enough to describe it, contributors will often catch their own mistakes before submitting them.
 - Cut and paste this checklist into the submission description, and indicate compliance with each point. By asking your contributors to copy your submission guidelines into the discussion of their submission, you get an immediate indication of whether they read the instructions; furthermore, asking people to indicate compliance with each point discourages contributors from ignoring any of the guidelines you've laid out.

If a contribution fails to respect any of the submission guidelines you lay out, look no further - you are well within your rights to politely but summarily insist these basic requirements are met before the contribution will be considered further.

### Summary

Before coding even begins, laying down some ground rules will improve the quality of code, and make your job as reviewer fast and easy:

 - Have a conversation with contributors beforehand, to ensure their plans make sense for the project.
 - Provide contributors with a checklist of submission guidelines that they should conform to before making a submission.
 - Demand strict adherence to the submission guidelines.


## Reviewing a Contribution

Once a collaborator has contributed a piece of code that passes muster in terms of the submission guidelines from the last lesson, it's time to dig into the code and review it in detail. If the submission guidelines have done their job, the objectives of the code under examination should be clear (by way of the initial discussion and annotations upon submission), and the volume of code to review should be both small (less than 400 lines), and arranged in short, digestible chunks. At this stage, there are a few things to keep in mind to make the process fast and efficient:

**Review early & review often.** [This study](http://arxiv.org/pdf/1407.5648v2.pdf) of code review in the sciences dramatically demonstrated that code reviewed long after it is written fails to produce useful results, and rarely yields anything but the most superficial of comments. Good code review requires clear communication and a strong rapport between collaborators; all the techniques discussed here are designed to make this process as efficient and easy as possible. Discuss and review code as it is being developed, and much more substantial results will be achieved.

**Review about 300-500 lines of code per hour.** [This study](http://smartbear.com/smartbear/media/pdfs/wp-cc-11-best-practices-of-peer-code-review.pdf) found that reviewing code faster than this resulted in dramatic losses in review effectiveness. This means that the largest submission we were willing to accept (400 lines) should take about one hour to review - and shorter submissions correspondingly less.

**Rely on your test suite.** The review stage is one place where having a set of unit tests really pays off. By running the automated tests as the very first step, you immediately and effortlessly confirm that the code is functioning according to expectations. By writing additional simple tests at this stage, not only does the review process become very robust, but you build up a library of checks that can be run in future to immediately ensure new contributions haven't broken old code.

**Have a checklist of things to look for.** One pitfall of reviewing any sort of work, is that there's rarely a clear indication of when the task is complete; another is that it's easy to call out problems with what's written, but harder to notice when things are absent. A well tuned checklist helps to circumscribe the review process, and reminds you to look for things both present and missing. As you review more code, you will begin to develop a set of patterns and red flags that appear commonly and indicate potential problems; but to start with, here are a number of generic things to keep an eye out for in any project. They split into two main categories: *intrinsic* considerations, and *extrinsic* ones.

**Intrinsic Examination** looks for elements internal to the new code underreview, without worrying too much about its context or the broader project. Some things to look for:

 - Are functions as simple as possible? We insisted in our submission guidelines that functions be short, but this is really a simple proxy for our true goal: to ensure that functions are modular. A modular function is one that does exactly one thing, and does it well - by insisting on breaking code up into the simplest possible functions, we make them easy to test, easy to understand, and easy to review. Can any of the functions in this new submission be broken up into simpler steps?
 - Is the code efficient? If some really slow operation is performed in the code, it should be performed as infrequently as possible. For example, diagonalizing a matrix can be a slow computation. If the new code needs to do a slow operation like this, it should do it once only, and store the result for use, rather than recomputing it every time it needs to use it. Not sure which operations are slow? Assume they all are! Don't repeat any operation unnecessarily - we call this well factored code.
 - Is the usage of each function clear? Try and put yourself in the mindset of someone coming to this code for the first time; how would they know how to re-use the existing code? The documentation for each function should have, at a minimum, a short description of the function, and a specification of its inputs and outputs.
 - Have edge cases been considered? Users will invariably do all kinds of things with your code that they aren't supposed to. If the user does something unexpected (atypical inputs, clicking on things they're not supposed to...), will this new code break, or will it roll with the punches? For example, if a function takes a numerical input, is there a situation where division by 0 could be triggered?

**Extrinsic Examination** considers how this new code is going to fit into the project as a whole. Some things to think about:

 - Does the new code reinvent any wheels? New code should be just that - new. If it's reproducing functionality you have elsewhere in the code, it shouldn't be repeated here. This is another reason for insisting on splitting functions up into small and reusable pieces; shared functionality can be written, reviewed and debugged just once, rather than over and over again in different contexts.
 - Does the new code successfully address the needs of the project? By sketching out that five-minute plan and having had a conversation with your contributors about what their new contribution is trying to address, you created a clear goal for this new work; does this contribution push the project forward by addressing those plans and discussions?
 - Does the new code respect the structure of the project? A healthy project is a well organized project with a high degree of standardization. If the rest of the code has established variable naming conventions, file structure, or other patterns, does this new code uphold those patterns?

### Flying Solo

Like any kind of review, code review is best done by a third party - how then do we take advantage of these techniques when we're working on a project alone, or when we are the sole maintainer of a project? Ideally, try to find a colleague to arrange some quid pro quo - reviewing each other's code, particularly if you are researching similar things, can be both educational and useful for everyone involved - you'll both have better code for having had it reviewed, and you'll both gain some insight into how someone else would solve a problem in your field.

But, if an arrangement like this simply isn't possible, much can still be done alone. The key practice in each step above, was setting expectations up front. By laying out a rough plan, setting standards for contribution, and making a checklist of things to look for in a new piece of code, you set a prior standard for your own work before you begin. Doing so gives you a more objective meter stick to assess your own success after the fact; without these standards articulated beforehand, it's much too tempting to assume something is 'good enough' and move on, once it is nominally complete. What's more, when making checklists of standards specifically for yourself, you can keep track of problems you've personally run into in the past, and double check that you aren't heading down that same road again this time around - this is a good habit to get into, regardless of whether you are working alone or with a team.

### Team Code Review

If like many scientists you are working in a small research group, consider adding a live code review element to your regular lab meetings. This is an excellent opportunity for everyone in your lab to learn from each other, by seeing how each other solves problems, and by helping everyone compose and refine their checklists of things to look out for when writing and reviewing code. It'll also help keep everyone abreast of the details of each other's work, exposing opportunities to save time and effort by sharing code.

A few special techniques to facilitate live code reviews (more details at this [excellent blog post](http://fperez.org/py4science/code_reviews.html)):

 - Keep it short. While an asynchronous code review can look at up to 400 lines of code, no one wants to sit through that in live discussion. Keep it to 100 lines or less, so that the process doesn't take too much time and focus from the group.
 - Keep it polite. It can be an uncomfortable situation to have the details of your work put under a semi-public microscope; whoever is leading the discussion is repsonsible for making sure feedback is constructive, and the person having their code reviewed feels like the conversation was useful. To achieve this, try framing the discussion around technique; let the author explain the strategy they employed to solve the problem at hand, and then let everyone else explain how they would have solved the problem. This way, the conversation is about sharing ideas, rather than calling people out. Insist that feedback is always specific and actionable, and a lot of bruised feelings can be avoided.
 - Make sure the goals are clear. Have whoever is presenting the code point it out a few days beforehand, and ask them to include indications of what the code's purpose is, in the form of a plan or roadmap for their project, a thread on an issue tracker, or just a description to the group well ahead of time. This way, participants have a more sophisticated understanding of what the code is supposed to do, which enables them to give deeper and more substantial feedback - rather than superficial comments about syntax.

### Legacy Code

In many cases, a large amount of code may have been written before a code review system has been put into place. Reviewing thousands of lines of pre-existing code is a monumental task - how to go about this without disrupting your schedule?

Above all, do not attempt to sit down and slog through the entire body of code. As cited above, successful code review depends on building a rapport between contributor and maintainer, where contributors have a clear understanding of the direction of the project, and reviewers have a deep understanding of what contributors are trying to do and how those efforts fit into the whole; all the techniques described above are designed to facillitate building that rapport and understanding quickly and efficiently - without it, communication breaks down, and review loses its efficacy.

So, while we can't immediately go back and review a massive amount of code, we can stop the unreviewed codebase from getting any further out of hand. Set up the standards and guidelines discussed above, and begin enforcing them for all new submissions; that way, at least the problem doesn't get any worse. In addition, require contributors to make small, incremental corrections to the pre-existing code as they use it; every time they use a function from the legacy code, ask them to write some minimal documentation and a test or two for that function. This divides the project of cleaning up old code into many small bites over many contributions, and ameliorates the problem over time without any single massive undertaking from any one individual. What's more, the exercise of slowly re-examining old code a little bit at a time will help everyone involved get a deeper understanding of the code, and gradually expose and eliminate bugs as they appear.

### Summary

 - Once a submission is recieved that meets the minimum standards for the project, use these simple guidelines to expedite & optimize the review process:
  - Review code soon after it is written, in managable chunks.
  - Review code at a rate of 300-500 lines of code per hour.
  - Take heavy advantage of your test suite.
  - Have a checklist of standards & red flags to keep in mind during a close reading of new code; remember to consider both intrinsic and extrinstic aspects of the code.

 - Setting the standards described above and self-checking for them is a useful exercise, even when working alone.

 - If feasible, try reviewing code live with your lab group; frame the disussion around sharing techniques, and make sure everyone knows what the purpose of the code is well beforehand, to encourage substantial comments.

 - The key to good code review is clear communication and a strong rapport between contributors. Therefore, it's crucial to discuss and review code in small pieces as it is written; do not attempt to review large amounts of code after the fact. When working with large amounts of pre-existing code, re-examine it only a little bit at a time, and employ your contributors to address its problems incrementally.
